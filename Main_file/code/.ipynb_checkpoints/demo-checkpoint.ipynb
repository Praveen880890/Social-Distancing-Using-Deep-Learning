{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple OpenVINO Optimized YOLOv8 Inferencing Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the previously prepared OpenVINO optimized YOLOv8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "core = Core()\n",
    "# Load the pre optimized model\n",
    "yolov8n_with_preprocess_model = core.read_model('models/yolov8n_openvino_int8_model/yolov8n_with_preprocess.xml',)\n",
    "\n",
    "\n",
    "import json\n",
    "# Load the label map\n",
    "with open('models/yolov8n_labels.json', 'r') as f:\n",
    "    label_map = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "Setup the live player for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from IPython import display\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, Tuple\n",
    "from ultralytics.yolo.utils.plotting import colors\n",
    "from utils import VideoPlayer, detect_without_preprocess\n",
    "\n",
    "distance_thres=100\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "\n",
    "#give the output address to store the video \n",
    "writer = cv2.VideoWriter('social_distance_openvino_optimized_version.mp4', fourcc, 30,(720,1280), True)\n",
    "\n",
    "def plot_one_box_new(box:np.ndarray, img:np.ndarray, color:Tuple[int, int, int] = None, mask:np.ndarray = None, label:str = None, line_thickness:int = 5):\n",
    "    \"\"\"\n",
    "    Helper function for drawing single bounding box on image\n",
    "    Parameters:\n",
    "        x (np.ndarray): bounding box coordinates in format [x1, y1, x2, y2]\n",
    "        img (no.ndarray): input image\n",
    "        color (Tuple[int, int, int], *optional*, None): color in BGR format for drawing box, if not specified will be selected randomly\n",
    "        mask (np.ndarray, *optional*, None): instance segmentation mask polygon in format [N, 2], where N - number of points in contour, if not provided, only box will be drawn\n",
    "        label (str, *optonal*, None): box label string, if not provided will not be provided as drowing result\n",
    "        line_thickness (int, *optional*, 5): thickness for box drawing lines\n",
    "    \"\"\"\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "    print(int(box[0]),int(box[1]),int(box[2]),int(box[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    # if label:\n",
    "    #     tf = max(tl - 1, 1)  # font thickness\n",
    "    #     t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    #     c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "    #     cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "    #     cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    # if mask is not None:\n",
    "    #     image_with_mask = img.copy()\n",
    "    #     mask\n",
    "    #     cv2.fillPoly(image_with_mask, pts=[mask.astype(int)], color=color)\n",
    "    #     img = cv2.addWeighted(img, 0.5, image_with_mask, 0.5, 1)\n",
    "    return img\n",
    "\n",
    "def draw_results_new(results:Dict, source_image:np.ndarray, label_map:Dict):\n",
    "    \"\"\"\n",
    "    Helper function for drawing bounding boxes on image\n",
    "    Parameters:\n",
    "        image_res (np.ndarray): detection predictions in format [x1, y1, x2, y2, score, label_id]\n",
    "        source_image (np.ndarray): input image for drawing\n",
    "        label_map; (Dict[int, str]): label_id to class name mapping\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    boxes = results[\"det\"]\n",
    "    masks = results.get(\"segment\")\n",
    "    h, w = source_image.shape[:2]\n",
    "    # print(\"####\",type(boxes.item()))\n",
    "    boxes=[t.numpy() for t in boxes]\n",
    "    persons = []\n",
    "    person_centres = []\n",
    "    violate = set()\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "            label = label_map[str(int(boxes[i][-1]))]\n",
    "            if label==\"person\":\n",
    "                x,y,w,h = tuple(boxes[i][:4])\n",
    "                persons.append(tuple(boxes[i][:4]))\n",
    "                person_centres.append([x+w//2,y+h//2])\n",
    "    # print(\"##\",len(persons))\n",
    "    # print(\"$%%%\",len(person_centres))\n",
    "    for i in range(len(persons)):\n",
    "         for j in range(i+1,len(persons)):\n",
    "                if dist(person_centres[i],person_centres[j]) <= distance_thres:\n",
    "                    violate.add(tuple(persons[i]))\n",
    "                    violate.add(tuple(persons[j]))\n",
    "    v = 0\n",
    "    for (x,y,w,h) in persons:\n",
    "        if (x,y,w,h) in violate:\n",
    "                color = (0,0,225)\n",
    "                v+=1\n",
    "        else:\n",
    "                    \n",
    "                color = (125,255,0)\n",
    "    \n",
    "        \n",
    "        tl = 1 or round(0.002 * (source_img.shape[0] + source_image.shape[1]) / 2) + 1  # line/font thickness\n",
    "        color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "        cv2.rectangle(source_image,(int(x),int(y)),(int(w),int(h)),color,thickness=tl, lineType=cv2.LINE_AA)\n",
    "        c1,c2=(int(x),int(y)),(int(w),int(h))\n",
    "        if label==\"person\":\n",
    "            tf = max(tl - 1, 1)  # font thickness\n",
    "            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "            \n",
    "            c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "            cv2.rectangle(source_image, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "            cv2.putText(source_image, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    cv2.putText(source_image,'Number of Violations : '+str(v),(80,source_image.shape[0]-10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "    return source_image\n",
    "# Caluculate the euclidean distance\n",
    "def dist(pt1,pt2):\n",
    "    try:\n",
    "        return ((pt1[0]-pt2[0])**2 + (pt1[1]-pt2[1])**2)**0.5\n",
    "    except:\n",
    "        return\n",
    "        \n",
    "# Run the object detection\n",
    "def run_object_detection(source=0, flip=False, use_popup=False, skip_first_frames=0, model=\"None\", device=\"None\"):\n",
    "    player = None\n",
    "    # if device != \"CPU\":\n",
    "    #     model.reshape({0: [1, 3, 640, 640]})\n",
    "    compiled_model = core.compile_model(model, device)\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "        out = cv2.VideoWriter('output.mp4', fourcc, 20.0, (1280,720))\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(\n",
    "                    src=frame,\n",
    "                    dsize=None,\n",
    "                    fx=scale,\n",
    "                    fy=scale,\n",
    "                    interpolation=cv2.INTER_AREA,\n",
    "                )\n",
    "            # Get the results.\n",
    "            input_image = np.array(frame)\n",
    "           \n",
    "            start_time = time.time()\n",
    "            # model expects RGB image, while video capturing in BGR\n",
    "            detections = detect_without_preprocess(input_image, compiled_model)[0]\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            image_with_boxes = draw_results_new(detections, input_image, label_map)\n",
    "            frame = image_with_boxes\n",
    "                \n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 900,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=3,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            \n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                out.write(frame)\n",
    "               \n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "                key = cv2.waitKey(1)\n",
    "            if key==ord(\"q\"):\n",
    "                \n",
    "                break\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            writer.release()\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source ended\n"
     ]
    }
   ],
   "source": [
    "# Start the video feed and run the object detection\n",
    "run_object_detection(source=\"pedestrians.mp4\", flip=False, use_popup=True,skip_first_frames=0, model=yolov8n_with_preprocess_model, device=\"AUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
