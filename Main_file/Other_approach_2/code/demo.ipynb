{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1de8a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import OpenVINO modules\n",
    "import openvino\n",
    "from openvino.inference_engine import IECore, IENetwork\n",
    "import openvino.runtime as ov\n",
    "import time\n",
    "\n",
    "# Load the IR model files\n",
    "model_xml = \"../models/person-detection-0202/FP32/person-detection-0202.xml\"\n",
    "model_bin = \"../models/person-detection-0202/FP32/person-detection-0202.bin\"\n",
    "\n",
    "ie = IECore()\n",
    "net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "\n",
    "# Get the input and output layer names\n",
    "input_blob = next(iter(net.input_info))\n",
    "output_blob = next(iter(net.outputs)) \n",
    "\n",
    "# Load the network to the device (CPU, GPU, etc.)\n",
    "core = ov.Core()\n",
    "model = core.compile_model(model_xml,\"CPU\")\n",
    "\n",
    "def dist(pt1,pt2):\n",
    "    try:\n",
    "        return ((pt1[0]-pt2[0])**2 + (pt1[1]-pt2[1])**2)**0.5\n",
    "    except:\n",
    "        return\n",
    "\n",
    "\n",
    "# Define a function to compute the Euclidean distance between two points\n",
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# Define a threshold for the minimum distance between people\n",
    "distance_threshold = 200 # pixels\n",
    "\n",
    "# Read and preprocess the input video\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#give the input video\n",
    "video = cv2.VideoCapture(\"../data/pedestrians.mp4\")\n",
    "_,frame = video.read()\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "#give the output address to store the video \n",
    "writer = cv2.VideoWriter('../demo_videos/output.avi', fourcc, 30,(width,height), True)\n",
    "\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    "\n",
    "distance_thres=100\n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    " \n",
    "while True:\n",
    "    # Read a frame from the video.\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break # Exit the loop if end of video or error\n",
    "    height, width = frame.shape[:2]\n",
    "   \n",
    "    # Preprocess the frame\n",
    "    image = cv2.resize(frame, (512,512))\n",
    "    image = image.transpose((2, 0, 1)) # Change data layout from HWC to CHW\n",
    "    \n",
    "    \n",
    "\n",
    "    # Run inference and get the output\n",
    "    infer_request = model.create_infer_request()\n",
    "    input_shape = [1,3,512,512]   \n",
    "    input_tensor= ov.Tensor(image.astype(np.float32))\n",
    "    input_tensor.shape = input_shape\n",
    "    infer_request.set_tensor(input_blob,input_tensor)\n",
    "    infer_request.start_async()\n",
    "    infer_request.wait()\n",
    "    output_tensor = infer_request.get_tensor(output_blob)\n",
    "    output = output_tensor.data\n",
    "    \n",
    "\n",
    "    # Parse the output and get the bounding boxes of detected people\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    new_frame_time = time.time()\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    fps = int(fps)\n",
    "    fps = str(fps)\n",
    "    \n",
    "    for detection in output[0][0]:\n",
    "        # Each detection has the format [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "        if detection[2] > 0.5: # Only keep detections with confidence > 0.5\n",
    "            class_id = int(detection[1])\n",
    "            if class_id == 0: # Only keep detections with label 0 (person)\n",
    "                x_min = int(detection[3] * width)\n",
    "                y_min = int(detection[4] * height)\n",
    "                x_max = int(detection[5] * width)\n",
    "                y_max = int(detection[6] * height)\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                confidences.append(float(detection[2]))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    n = len(boxes)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    label=\"person\"\n",
    "    \n",
    "\n",
    "    v = 0\n",
    "    person_centers=[]\n",
    "    persons=[]\n",
    "    violate=set()\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        # Draw a bounding box around the person\n",
    "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "        # Get the center point of the box\n",
    "        center_a = np.array([box[0] + (box[2] - box[0]) / 2, box[1] + (box[3] - box[1]) / 2])\n",
    "        persons.append(boxes[i])\n",
    "        person_centers.append(center_a)\n",
    "\n",
    "    for i in range(len(persons)):\n",
    "        for j in range(i+1,len(persons)):\n",
    "            distance = euclidean_distance(person_centers[i], person_centers[j])\n",
    "            if distance < distance_threshold:\n",
    "                violate.add(tuple(persons[i]))\n",
    "                violate.add(tuple(persons[j]))\n",
    "    \n",
    "    for (x,y,w,h) in persons:\n",
    "            # print((x,y,w,h))\n",
    "            if tuple((x,y,w,h)) in violate:\n",
    "                    color = (0,0,225)\n",
    "                    v+=1\n",
    "                    # print(\"YES\")\n",
    "            else:\n",
    "                    \n",
    "                    color = (125,255,0)\n",
    "    \n",
    "            cv2.rectangle(frame,(x,y),(w,h),color,2)\n",
    "            tl = 2 or round(0.002 * (frame.shape[0] + source_image.shape[1]) / 2) + 1  # line/font thickness\n",
    "            color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "            cv2.rectangle(frame,(int(x),int(y)),(int(w),int(h)),color,thickness=tl, lineType=cv2.LINE_AA)\n",
    "            c1,c2=(int(x),int(y)),(int(w),int(h))\n",
    "            if label==\"person\":\n",
    "                tf = max(tl - 1, 1)  # font thickness\n",
    "                t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "                \n",
    "                c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "                cv2.rectangle(frame, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "                cv2.putText(frame, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "            \n",
    "    # # Show the output frame\n",
    "    cv2.namedWindow(\"Social Distance Detector\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Social Distance Detector\", 800, 600)\n",
    "    cv2.putText(frame,'Number of Violations : '+str(v),(80,frame.shape[0]-10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "    \n",
    "    cv2.putText(frame,\"FPS :\"+ fps, (7,70), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,255), 3, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Social Distance Detector\", frame)\n",
    "    writer.write(frame)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Wait for a key press to exit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release the video and destroy the windows\n",
    "video.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8fece-a1ab-457c-aa00-d33b92696f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560181a8-ac89-471f-ad87-4219bdf7dd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4dddd-a600-45f6-ae5b-8b832d1052fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea76a0-504e-46c4-b114-8e549363b78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7e23e-b96e-4217-83ba-45f530d35182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbaff2-2459-4bb8-b9b3-9d289d7a5914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb48eb7f-c5d9-423b-ace0-620871a4e94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926303d6-a2ec-49ca-9c6b-49716125af52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a9e4b-fb57-42da-92c5-9988441e43d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee2eae-d87a-4f10-9d21-59c2ef15a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c442758-161d-4831-b2bb-0b57d44a7978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c528ed4-57cb-4edf-a829-2da2dfc78bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e809a-7df3-49ac-8eea-d88066581f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c0a39-11f8-4270-90be-6c82c84dfb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5915ab-6832-40b0-8979-c76927d0af38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
